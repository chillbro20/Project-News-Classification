{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize \n",
    "    # nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('dataset/news.json', lines=True,)\n",
    "data.drop(['authors','link','date'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text'] = data['headline'] + data['short_description']\n",
    "data.drop(['headline','short_description'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               Text\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...\n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...\n",
       "2  ENTERTAINMENT  Hugh Grant Marries For The First Time At Age 5...\n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...\n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "#Handle Multiclass Imbalance Datasets\n",
    "def Imbalance_to_balance(df,No_of_sample):\n",
    "    \n",
    "    # Dict used to store data class wise\n",
    "    df_dic = {}\n",
    "    \n",
    "    # Create dataframe to store data after sampling\n",
    "    df_new = pd.DataFrame(columns=['category','Text'])\n",
    "    \n",
    "    #cal use to iterate over each class in target col\n",
    "    for cal in df['category'].unique():\n",
    "        \n",
    "        # Filtering class from target which has more than No_of_sample row\n",
    "        if df[df['category'] == cal].shape[0] > No_of_sample:\n",
    "            \n",
    "            #Extracting 4000 sample class wise from dataframe\n",
    "            df_dic[cal] = df[df['category'] == cal].sample(No_of_sample,random_state=42,ignore_index=True)\n",
    "    \n",
    "    # store all class which has more than 4000 row\n",
    "    cal = list(df_dic.keys())\n",
    "    for classs in cal:\n",
    "        \n",
    "        #concate data of each class into new data frame\n",
    "        df_new = pd.concat([df_new,df_dic[f\"{classs}\"]],axis=0)\n",
    "    \n",
    "    return shuffle(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= Imbalance_to_balance(data,8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.head(500).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6056    Extended Breastfeeding -- Time to Come Out of ...\n",
       "309     What Not to Say to a Woman After a Miscarriage...\n",
       "6885    Interview With Nick Frost on Cuban Fury, Wrapp...\n",
       "3698    The Best Cartoon Character Baby NamesToday is ...\n",
       "4349    12 Lessons My Twins Taught Me In Their First 1...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PARENTING         8000\n",
       "ENTERTAINMENT     8000\n",
       "WELLNESS          8000\n",
       "STYLE & BEAUTY    8000\n",
       "TRAVEL            8000\n",
       "POLITICS          8000\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = df['Text'].apply(lambda x:len(x.split())).max()\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def process_text(text):\n",
    "    text = text.lower().replace('\\n',' ').replace('\\r','').strip()\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(r'[0-9]','',text)\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "    \n",
    "    text = \" \".join(filtered_sentence)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(lambda x:process_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = df['Text'].apply(lambda x:len(x.split())).max()\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dic = {}\n",
    "for key, label in enumerate(df['category'].unique()):\n",
    "    label_dic[label] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PARENTING': 0, 'ENTERTAINMENT': 1, 'WELLNESS': 2, 'STYLE & BEAUTY': 3, 'TRAVEL': 4, 'POLITICS': 5}\n"
     ]
    }
   ],
   "source": [
    "print(label_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Building Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomTreesEmbedding\n",
    "from xgboost import XGBRFClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "models = []\n",
    "models.append((\"DecisionTree\",DecisionTreeClassifier()))\n",
    "models.append((\"RandomForest\",RandomForestClassifier()))\n",
    "models.append((\"ExtraTreeClassifier\",ExtraTreeClassifier()))\n",
    "models.append((\"XGBRFClassifier\",XGBRFClassifier()))\n",
    "models.append((\"XGBClassifier\",XGBClassifier()))\n",
    "models.append((\"ExtraTreeClassifier\",ExtraTreeClassifier()))\n",
    "models.append((\"LinearSVC\",LinearSVC()))\n",
    "models.append((\"KNeighbors\",KNeighborsClassifier()))\n",
    "\n",
    "\n",
    "# results = []\n",
    "# namwarnings[]\n",
    "# for name,model in models:\n",
    "#     result = cross_val_score(model, X, y,  cv=5)\n",
    "#     names.append(name)\n",
    "#     results.append(result)\n",
    "\n",
    "# for i in range(len(names)):\n",
    "#     print(names[i],results[i].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = df['Text'].apply(lambda x:len(x.split())).max()\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "MAX_NB_WORDS = 1000\n",
    "max_len = int(round(df['Text'].apply(lambda x: len(str(x).split())).max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF_ML(X,y):\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    #print(\"Vocabulary Size :\", vocab_size)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30, random_state=42)\n",
    "    \n",
    "    \n",
    "    X_train = pad_sequences(tokenizer.texts_to_sequences(X_train),\n",
    "                        maxlen = max_len)\n",
    "    X_test = pad_sequences(tokenizer.texts_to_sequences(X_test),\n",
    "                       maxlen = max_len)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = TF_IDF_ML(df['Text'],df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def Train_Model(model):\n",
    "    print(\"training:\",model)\n",
    "    print(\"xtrain\",X_train.dtype)\n",
    "    print(\"xtest\",X_test.dtype)\n",
    "    print(\"ytrain\",y_train.dtype)\n",
    "    print(\"ytest\",y_test.dtype)\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    return classification_report(y_test,model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "y_train = enc.fit_transform(y_train)\n",
    "y_test = enc.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree\n",
      "training: DecisionTreeClassifier()\n",
      "xtrain int32\n",
      "xtest int32\n",
      "ytrain int32\n",
      "ytest int32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.35      0.35      2396\n",
      "           1       0.25      0.26      0.25      2444\n",
      "           2       0.30      0.29      0.29      2458\n",
      "           3       0.30      0.30      0.30      2336\n",
      "           4       0.21      0.21      0.21      2379\n",
      "           5       0.25      0.25      0.25      2387\n",
      "\n",
      "    accuracy                           0.28     14400\n",
      "   macro avg       0.28      0.28      0.28     14400\n",
      "weighted avg       0.28      0.28      0.28     14400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RandomForest\n",
      "training: RandomForestClassifier()\n",
      "xtrain int32\n",
      "xtest int32\n",
      "ytrain int32\n",
      "ytest int32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.52      0.51      2396\n",
      "           1       0.34      0.36      0.35      2444\n",
      "           2       0.42      0.34      0.37      2458\n",
      "           3       0.35      0.49      0.41      2336\n",
      "           4       0.30      0.21      0.25      2379\n",
      "           5       0.30      0.30      0.30      2387\n",
      "\n",
      "    accuracy                           0.37     14400\n",
      "   macro avg       0.37      0.37      0.36     14400\n",
      "weighted avg       0.37      0.37      0.36     14400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ExtraTreeClassifier\n",
      "training: ExtraTreeClassifier()\n",
      "xtrain int32\n",
      "xtest int32\n",
      "ytrain int32\n",
      "ytest int32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.34      0.33      2396\n",
      "           1       0.24      0.23      0.23      2444\n",
      "           2       0.26      0.25      0.25      2458\n",
      "           3       0.25      0.26      0.26      2336\n",
      "           4       0.19      0.19      0.19      2379\n",
      "           5       0.22      0.23      0.23      2387\n",
      "\n",
      "    accuracy                           0.25     14400\n",
      "   macro avg       0.25      0.25      0.25     14400\n",
      "weighted avg       0.25      0.25      0.25     14400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "XGBRFClassifier\n",
      "training: XGBRFClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                colsample_bylevel=None, colsample_bytree=None,\n",
      "                early_stopping_rounds=None, enable_categorical=False,\n",
      "                eval_metric=None, gamma=None, gpu_id=None, grow_policy=None,\n",
      "                importance_type=None, interaction_constraints=None,\n",
      "                max_bin=None, max_cat_to_onehot=None, max_delta_step=None,\n",
      "                max_depth=None, max_leaves=None, min_child_weight=None,\n",
      "                missing=nan, monotone_constraints=None, n_estimators=100,\n",
      "                n_jobs=None, num_parallel_tree=None,\n",
      "                objective='binary:logistic', predictor=None, random_state=None,\n",
      "                reg_alpha=None, sampling_method=None, scale_pos_weight=None, ...)\n",
      "xtrain int32\n",
      "xtest int32\n",
      "ytrain int32\n",
      "ytest int32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.54      0.50      2396\n",
      "           1       0.32      0.23      0.27      2444\n",
      "           2       0.35      0.29      0.31      2458\n",
      "           3       0.31      0.56      0.40      2336\n",
      "           4       0.31      0.14      0.19      2379\n",
      "           5       0.30      0.31      0.30      2387\n",
      "\n",
      "    accuracy                           0.34     14400\n",
      "   macro avg       0.34      0.35      0.33     14400\n",
      "weighted avg       0.34      0.34      0.33     14400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "XGBClassifier\n",
      "training: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=None,\n",
      "              reg_alpha=None, reg_lambda=None, ...)\n",
      "xtrain int32\n",
      "xtest int32\n",
      "ytrain int32\n",
      "ytest int32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.54      0.52      2396\n",
      "           1       0.45      0.46      0.45      2444\n",
      "           2       0.51      0.45      0.48      2458\n",
      "           3       0.50      0.57      0.53      2336\n",
      "           4       0.39      0.32      0.35      2379\n",
      "           5       0.36      0.39      0.38      2387\n",
      "\n",
      "    accuracy                           0.45     14400\n",
      "   macro avg       0.45      0.45      0.45     14400\n",
      "weighted avg       0.45      0.45      0.45     14400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ExtraTreeClassifier\n",
      "training: ExtraTreeClassifier()\n",
      "xtrain int32\n",
      "xtest int32\n",
      "ytrain int32\n",
      "ytest int32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.35      0.34      2396\n",
      "           1       0.24      0.23      0.23      2444\n",
      "           2       0.28      0.26      0.27      2458\n",
      "           3       0.27      0.27      0.27      2336\n",
      "           4       0.19      0.19      0.19      2379\n",
      "           5       0.23      0.24      0.24      2387\n",
      "\n",
      "    accuracy                           0.26     14400\n",
      "   macro avg       0.26      0.26      0.26     14400\n",
      "weighted avg       0.26      0.26      0.26     14400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LinearSVC\n",
      "training: LinearSVC()\n",
      "xtrain int32\n",
      "xtest int32\n",
      "ytrain int32\n",
      "ytest int32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erris\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.26      0.24      2396\n",
      "           1       0.24      0.14      0.18      2444\n",
      "           2       0.21      0.29      0.24      2458\n",
      "           3       0.19      0.27      0.22      2336\n",
      "           4       0.16      0.17      0.16      2379\n",
      "           5       0.22      0.10      0.14      2387\n",
      "\n",
      "    accuracy                           0.20     14400\n",
      "   macro avg       0.21      0.20      0.20     14400\n",
      "weighted avg       0.21      0.20      0.20     14400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "KNeighbors\n",
      "training: KNeighborsClassifier()\n",
      "xtrain int32\n",
      "xtest int32\n",
      "ytrain int32\n",
      "ytest int32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.48      0.40      2396\n",
      "           1       0.26      0.35      0.30      2444\n",
      "           2       0.29      0.27      0.28      2458\n",
      "           3       0.30      0.24      0.27      2336\n",
      "           4       0.22      0.14      0.17      2379\n",
      "           5       0.25      0.21      0.23      2387\n",
      "\n",
      "    accuracy                           0.28     14400\n",
      "   macro avg       0.28      0.28      0.27     14400\n",
      "weighted avg       0.28      0.28      0.27     14400\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model[0])\n",
    "    print(Train_Model(model[1]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, ..., 1, 5, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.54      0.52      2396\n",
      "           1       0.45      0.46      0.45      2444\n",
      "           2       0.51      0.45      0.48      2458\n",
      "           3       0.50      0.57      0.53      2336\n",
      "           4       0.39      0.32      0.35      2379\n",
      "           5       0.36      0.39      0.38      2387\n",
      "\n",
      "    accuracy                           0.45     14400\n",
      "   macro avg       0.45      0.45      0.45     14400\n",
      "weighted avg       0.45      0.45      0.45     14400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_XGBC = XGBClassifier()\n",
    "model_XGBC.fit(X_train,y_train)\n",
    "print(classification_report(y_test,model_XGBC.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4\n",
       "0    1\n",
       "0    3\n",
       "0    2\n",
       "0    0\n",
       "0    5\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df['category'].map(label_dic)\n",
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n",
    "    return labeled\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Text'],labels, random_state=0, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['completely', 'normal', 'living', 'mental', 'illnessno', 'one', 'deserves', 'live', 'stigma', 'associated', 'mental', 'illness', 'important', 'thing', 'need', 'society', 'compassionate', 'informed', 'caring', 'supportive', 'mental', 'illness'], tags=['Train_0']),\n",
       " TaggedDocument(words=['teen', 'wolf', 'iedthis', 'week', 'teen', 'wolf', 'eyes', 'new', 'deadly', 'assassins', 'invaded', 'beacon', 'hills', 'high', 'school', 'hunt', 'names', 'benefactors', 'list', 'ill', 'admit', 'tvtag'], tags=['Train_1'])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['completely', 'normal', 'living', 'mental', 'illnessno', 'one', 'deserves', 'live', 'stigma', 'associated', 'mental', 'illness', 'important', 'thing', 'need', 'society', 'compassionate', 'informed', 'caring', 'supportive', 'mental', 'illness'], tags=['Train_0'])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6056</th>\n",
       "      <td>PARENTING</td>\n",
       "      <td>extended breastfeeding time come closetnot eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>PARENTING</td>\n",
       "      <td>say woman miscarriagei recently miscarriage go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6885</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>interview nick frost cuban fury wrapping corne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3698</th>\n",
       "      <td>PARENTING</td>\n",
       "      <td>best cartoon character baby namestoday donald ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4349</th>\n",
       "      <td>PARENTING</td>\n",
       "      <td>lessons twins taught first months time preciou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           category                                               Text\n",
       "6056      PARENTING  extended breastfeeding time come closetnot eve...\n",
       "309       PARENTING  say woman miscarriagei recently miscarriage go...\n",
       "6885  ENTERTAINMENT  interview nick frost cuban fury wrapping corne...\n",
       "3698      PARENTING  best cartoon character baby namestoday donald ...\n",
       "4349      PARENTING  lessons twins taught first months time preciou..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_lengh = df['Text'].apply(lambda x:len(x.split())).max()\n",
    "max_lengh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48000/48000 [00:00<00:00, 976886.76it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1430496.11it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 911363.57it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1597601.87it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1447039.40it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1651463.33it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1443160.00it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1144772.68it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1500891.56it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 745469.06it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1426613.80it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1402474.33it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1531315.11it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1442818.69it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1544105.04it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1534337.74it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 944809.36it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 960275.65it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 989441.42it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1728882.10it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1387292.02it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1501417.63it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1503222.52it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1023916.67it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1012362.93it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 2006344.03it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1242135.67it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1256194.92it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1163771.16it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1236406.80it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1434103.30it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=max_lengh, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x17a95fd45e0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erris\\AppData\\Local\\Temp\\ipykernel_3644\\504554781.py:13: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  vectors[i] = model.docvecs[prefix]\n"
     ]
    }
   ],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors\n",
    "    \n",
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), max_lengh, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), max_lengh, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def Train_Model(model):\n",
    "    model.fit(train_vectors_dbow,y_train)\n",
    "    return print(classification_report(y_test,model.predict(test_vectors_dbow)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.38      0.39      2431\n",
      "           1       0.42      0.44      0.43      2367\n",
      "           2       0.41      0.41      0.41      2403\n",
      "           3       0.50      0.48      0.49      2448\n",
      "           4       0.46      0.46      0.46      2384\n",
      "           5       0.56      0.60      0.58      2367\n",
      "\n",
      "    accuracy                           0.46     14400\n",
      "   macro avg       0.46      0.46      0.46     14400\n",
      "weighted avg       0.46      0.46      0.46     14400\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "RandomForest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.70      0.70      2431\n",
      "           1       0.73      0.71      0.72      2367\n",
      "           2       0.71      0.71      0.71      2403\n",
      "           3       0.81      0.78      0.79      2448\n",
      "           4       0.77      0.77      0.77      2384\n",
      "           5       0.80      0.85      0.82      2367\n",
      "\n",
      "    accuracy                           0.75     14400\n",
      "   macro avg       0.75      0.75      0.75     14400\n",
      "weighted avg       0.75      0.75      0.75     14400\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "ExtraTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.31      0.31      2431\n",
      "           1       0.35      0.39      0.37      2367\n",
      "           2       0.35      0.33      0.34      2403\n",
      "           3       0.41      0.39      0.40      2448\n",
      "           4       0.37      0.38      0.37      2384\n",
      "           5       0.47      0.48      0.47      2367\n",
      "\n",
      "    accuracy                           0.38     14400\n",
      "   macro avg       0.38      0.38      0.38     14400\n",
      "weighted avg       0.38      0.38      0.38     14400\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "XGBRFClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.59      0.59      2431\n",
      "           1       0.60      0.64      0.62      2367\n",
      "           2       0.60      0.59      0.60      2403\n",
      "           3       0.69      0.65      0.67      2448\n",
      "           4       0.66      0.63      0.64      2384\n",
      "           5       0.71      0.77      0.74      2367\n",
      "\n",
      "    accuracy                           0.64     14400\n",
      "   macro avg       0.64      0.64      0.64     14400\n",
      "weighted avg       0.64      0.64      0.64     14400\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "XGBClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.75      0.75      2431\n",
      "           1       0.77      0.77      0.77      2367\n",
      "           2       0.74      0.74      0.74      2403\n",
      "           3       0.84      0.81      0.83      2448\n",
      "           4       0.81      0.83      0.82      2384\n",
      "           5       0.85      0.85      0.85      2367\n",
      "\n",
      "    accuracy                           0.79     14400\n",
      "   macro avg       0.79      0.79      0.79     14400\n",
      "weighted avg       0.79      0.79      0.79     14400\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "ExtraTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.33      0.34      2431\n",
      "           1       0.37      0.39      0.38      2367\n",
      "           2       0.32      0.31      0.32      2403\n",
      "           3       0.39      0.38      0.38      2448\n",
      "           4       0.38      0.37      0.37      2384\n",
      "           5       0.43      0.44      0.44      2367\n",
      "\n",
      "    accuracy                           0.37     14400\n",
      "   macro avg       0.37      0.37      0.37     14400\n",
      "weighted avg       0.37      0.37      0.37     14400\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "LinearSVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erris\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.75      0.76      2431\n",
      "           1       0.77      0.78      0.77      2367\n",
      "           2       0.77      0.74      0.75      2403\n",
      "           3       0.84      0.83      0.84      2448\n",
      "           4       0.80      0.83      0.82      2384\n",
      "           5       0.84      0.87      0.85      2367\n",
      "\n",
      "    accuracy                           0.80     14400\n",
      "   macro avg       0.80      0.80      0.80     14400\n",
      "weighted avg       0.80      0.80      0.80     14400\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "KNeighbors\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.28      0.40      2431\n",
      "           1       0.37      0.85      0.52      2367\n",
      "           2       0.86      0.33      0.48      2403\n",
      "           3       0.95      0.34      0.50      2448\n",
      "           4       0.63      0.84      0.72      2384\n",
      "           5       0.69      0.91      0.79      2367\n",
      "\n",
      "    accuracy                           0.59     14400\n",
      "   macro avg       0.70      0.59      0.57     14400\n",
      "weighted avg       0.71      0.59      0.57     14400\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model[0])\n",
    "    print(Train_Model(model[1]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DecisionTree', DecisionTreeClassifier()),\n",
       " ('RandomForest', RandomForestClassifier()),\n",
       " ('ExtraTreeClassifier', ExtraTreeClassifier()),\n",
       " ('XGBRFClassifier',\n",
       "  XGBRFClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "                  colsample_bylevel=1, colsample_bytree=1,\n",
       "                  early_stopping_rounds=None, enable_categorical=False,\n",
       "                  eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "                  importance_type=None, interaction_constraints='', max_bin=256,\n",
       "                  max_cat_to_onehot=4, max_delta_step=0, max_depth=6,\n",
       "                  max_leaves=0, min_child_weight=1, missing=nan,\n",
       "                  monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "                  num_parallel_tree=100, objective='multi:softprob',\n",
       "                  predictor='auto', random_state=0, reg_alpha=0,\n",
       "                  sampling_method='uniform', scale_pos_weight=None, ...)),\n",
       " ('XGBClassifier',\n",
       "  XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "                colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "                early_stopping_rounds=None, enable_categorical=False,\n",
       "                eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "                importance_type=None, interaction_constraints='',\n",
       "                learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "                max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "                missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "                n_jobs=0, num_parallel_tree=1, objective='multi:softprob',\n",
       "                predictor='auto', random_state=0, reg_alpha=0, ...)),\n",
       " ('ExtraTreeClassifier', ExtraTreeClassifier()),\n",
       " ('LinearSVC', LinearSVC()),\n",
       " ('KNeighbors', KNeighborsClassifier())]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'max_len':max_len,\n",
    "    'tokenizer':tokenizer,\n",
    "    'encoder':enc,\n",
    "    'vectorzier':vectorizer,\n",
    "    'models':models,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models.pk']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(model_dict,\"models.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = load('models.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_len': 128,\n",
       " 'tokenizer': <keras_preprocessing.text.Tokenizer at 0x17abb13c520>,\n",
       " 'encoder': LabelEncoder(),\n",
       " 'vectorzier': TfidfVectorizer(),\n",
       " 'models': [('DecisionTree', DecisionTreeClassifier()),\n",
       "  ('RandomForest', RandomForestClassifier()),\n",
       "  ('ExtraTreeClassifier', ExtraTreeClassifier()),\n",
       "  ('XGBRFClassifier',\n",
       "   XGBRFClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "                   colsample_bylevel=1, colsample_bytree=1,\n",
       "                   early_stopping_rounds=None, enable_categorical=False,\n",
       "                   eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "                   importance_type=None, interaction_constraints='', max_bin=256,\n",
       "                   max_cat_to_onehot=4, max_delta_step=0, max_depth=6,\n",
       "                   max_leaves=0, min_child_weight=1, missing=nan,\n",
       "                   monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "                   num_parallel_tree=100, objective='multi:softprob',\n",
       "                   predictor='auto', random_state=0, reg_alpha=0,\n",
       "                   sampling_method='uniform', scale_pos_weight=None, ...)),\n",
       "  ('XGBClassifier',\n",
       "   XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "                 colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "                 early_stopping_rounds=None, enable_categorical=False,\n",
       "                 eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "                 importance_type=None, interaction_constraints='',\n",
       "                 learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "                 max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "                 missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "                 n_jobs=0, num_parallel_tree=1, objective='multi:softprob',\n",
       "                 predictor='auto', random_state=0, reg_alpha=0, ...)),\n",
       "  ('ExtraTreeClassifier', ExtraTreeClassifier()),\n",
       "  ('LinearSVC', LinearSVC()),\n",
       "  ('KNeighbors', KNeighborsClassifier())]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b8a52038f5da5aa877987515cd692d8371a6f143b75c12d63db471be6b9a2e4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
