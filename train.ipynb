{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize \n",
    "    # nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('dataset/news.json', lines=True,)\n",
    "data.drop(['authors','link','date'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text'] = data['headline'] + data['short_description']\n",
    "data.drop(['headline','short_description'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               Text\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...\n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...\n",
       "2  ENTERTAINMENT  Hugh Grant Marries For The First Time At Age 5...\n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...\n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "#Handle Multiclass Imbalance Datasets\n",
    "def Imbalance_to_balance(df,No_of_sample):\n",
    "    \n",
    "    # Dict used to store data class wise\n",
    "    df_dic = {}\n",
    "    \n",
    "    # Create dataframe to store data after sampling\n",
    "    df_new = pd.DataFrame(columns=['category','Text'])\n",
    "    \n",
    "    #cal use to iterate over each class in target col\n",
    "    for cal in df['category'].unique():\n",
    "        \n",
    "        # Filtering class from target which has more than No_of_sample row\n",
    "        if df[df['category'] == cal].shape[0] > No_of_sample:\n",
    "            \n",
    "            #Extracting 4000 sample class wise from dataframe\n",
    "            df_dic[cal] = df[df['category'] == cal].sample(No_of_sample,random_state=42,ignore_index=True)\n",
    "    \n",
    "    # store all class which has more than 4000 row\n",
    "    cal = list(df_dic.keys())\n",
    "    for classs in cal:\n",
    "        \n",
    "        #concate data of each class into new data frame\n",
    "        df_new = pd.concat([df_new,df_dic[f\"{classs}\"]],axis=0)\n",
    "    \n",
    "    return shuffle(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= Imbalance_to_balance(data,8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.head(500).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4548    Romney, Rubio And Many Others Have Called Trum...\n",
       "2591    My Personal Playlist for Spring 2013When I am ...\n",
       "6613    Obama Family Attends Church In Matching Monoch...\n",
       "1308    Blac Chyna And Rob Kardashian Are 'Taking Thin...\n",
       "7775    Jennifer Aniston Gets Dolled Up By Ellen DeGen...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POLITICS          8000\n",
       "WELLNESS          8000\n",
       "STYLE & BEAUTY    8000\n",
       "ENTERTAINMENT     8000\n",
       "PARENTING         8000\n",
       "TRAVEL            8000\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = df['Text'].apply(lambda x:len(x.split())).max()\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def process_text(text):\n",
    "    text = text.lower().replace('\\n',' ').replace('\\r','').strip()\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(r'[0-9]','',text)\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "    \n",
    "    text = \" \".join(filtered_sentence)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(lambda x:process_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = df['Text'].apply(lambda x:len(x.split())).max()\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dic = {}\n",
    "for key, label in enumerate(df['category'].unique()):\n",
    "    label_dic[label] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'POLITICS': 0, 'WELLNESS': 1, 'STYLE & BEAUTY': 2, 'ENTERTAINMENT': 3, 'PARENTING': 4, 'TRAVEL': 5}\n"
     ]
    }
   ],
   "source": [
    "print(label_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Building Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomTreesEmbedding\n",
    "from xgboost import XGBRFClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "models = []\n",
    "models.append((\"DecisionTree\",DecisionTreeClassifier()))\n",
    "models.append((\"RandomForest\",RandomForestClassifier()))\n",
    "models.append((\"ExtraTreeClassifier\",ExtraTreeClassifier()))\n",
    "models.append((\"XGBRFClassifier\",XGBRFClassifier()))\n",
    "models.append((\"XGBClassifier\",XGBClassifier()))\n",
    "models.append((\"ExtraTreeClassifier\",ExtraTreeClassifier()))\n",
    "models.append((\"LinearSVC\",LinearSVC()))\n",
    "models.append((\"KNeighbors\",KNeighborsClassifier()))\n",
    "\n",
    "\n",
    "# results = []\n",
    "# namwarnings[]\n",
    "# for name,model in models:\n",
    "#     result = cross_val_score(model, X, y,  cv=5)\n",
    "#     names.append(name)\n",
    "#     results.append(result)\n",
    "\n",
    "# for i in range(len(names)):\n",
    "#     print(names[i],results[i].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = df['Text'].apply(lambda x:len(x.split())).max()\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "MAX_NB_WORDS = 1000\n",
    "max_len = int(round(df['Text'].apply(lambda x: len(str(x).split())).max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF_ML(X,y):\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    #print(\"Vocabulary Size :\", vocab_size)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30, random_state=42)\n",
    "    \n",
    "    \n",
    "    X_train = pad_sequences(tokenizer.texts_to_sequences(X_train),\n",
    "                        maxlen = max_len)\n",
    "    X_test = pad_sequences(tokenizer.texts_to_sequences(X_test),\n",
    "                       maxlen = max_len)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = TF_IDF_ML(df['Text'],df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def Train_Model(model):\n",
    "    print(\"training:\",model)\n",
    "    print(\"xtrain\",X_train.dtype)\n",
    "    print(\"xtest\",X_test.dtype)\n",
    "    print(\"ytrain\",y_train.dtype)\n",
    "    print(\"ytest\",y_test.dtype)\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    return classification_report(y_test,model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "y_train = enc.fit_transform(y_train)\n",
    "y_test = enc.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree\n",
      "training: DecisionTreeClassifier()\n",
      "xtrain int32\n",
      "xtest int32\n",
      "ytrain int32\n",
      "ytest int32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.35      0.35      2453\n",
      "           1       0.24      0.25      0.25      2357\n",
      "           2       0.28      0.28      0.28      2453\n",
      "           3       0.30      0.31      0.30      2364\n",
      "           4       0.21      0.20      0.21      2421\n",
      "           5       0.24      0.23      0.23      2352\n",
      "\n",
      "    accuracy                           0.27     14400\n",
      "   macro avg       0.27      0.27      0.27     14400\n",
      "weighted avg       0.27      0.27      0.27     14400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RandomForest\n",
      "training: RandomForestClassifier()\n",
      "xtrain int32\n",
      "xtest int32\n",
      "ytrain int32\n",
      "ytest int32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.51      0.51      2453\n",
      "           1       0.31      0.35      0.33      2357\n",
      "           2       0.39      0.33      0.36      2453\n",
      "           3       0.37      0.49      0.42      2364\n",
      "           4       0.32      0.22      0.26      2421\n",
      "           5       0.28      0.29      0.29      2352\n",
      "\n",
      "    accuracy                           0.37     14400\n",
      "   macro avg       0.36      0.37      0.36     14400\n",
      "weighted avg       0.37      0.37      0.36     14400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ExtraTreeClassifier\n",
      "training: ExtraTreeClassifier()\n",
      "xtrain int32\n",
      "xtest int32\n",
      "ytrain int32\n",
      "ytest int32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.34      0.34      2453\n",
      "           1       0.21      0.22      0.22      2357\n",
      "           2       0.26      0.25      0.25      2453\n",
      "           3       0.26      0.26      0.26      2364\n",
      "           4       0.20      0.19      0.20      2421\n",
      "           5       0.23      0.24      0.24      2352\n",
      "\n",
      "    accuracy                           0.25     14400\n",
      "   macro avg       0.25      0.25      0.25     14400\n",
      "weighted avg       0.25      0.25      0.25     14400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "XGBRFClassifier\n",
      "training: XGBRFClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                colsample_bylevel=None, colsample_bytree=None,\n",
      "                early_stopping_rounds=None, enable_categorical=False,\n",
      "                eval_metric=None, gamma=None, gpu_id=None, grow_policy=None,\n",
      "                importance_type=None, interaction_constraints=None,\n",
      "                max_bin=None, max_cat_to_onehot=None, max_delta_step=None,\n",
      "                max_depth=None, max_leaves=None, min_child_weight=None,\n",
      "                missing=nan, monotone_constraints=None, n_estimators=100,\n",
      "                n_jobs=None, num_parallel_tree=None,\n",
      "                objective='binary:logistic', predictor=None, random_state=None,\n",
      "                reg_alpha=None, sampling_method=None, scale_pos_weight=None, ...)\n",
      "xtrain int32\n",
      "xtest int32\n",
      "ytrain int32\n",
      "ytest int32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.56      0.52      2453\n",
      "           1       0.29      0.26      0.27      2357\n",
      "           2       0.35      0.28      0.31      2453\n",
      "           3       0.32      0.56      0.41      2364\n",
      "           4       0.33      0.14      0.20      2421\n",
      "           5       0.30      0.30      0.30      2352\n",
      "\n",
      "    accuracy                           0.35     14400\n",
      "   macro avg       0.35      0.35      0.34     14400\n",
      "weighted avg       0.35      0.35      0.34     14400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "XGBClassifier\n",
      "training: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=None,\n",
      "              reg_alpha=None, reg_lambda=None, ...)\n",
      "xtrain int32\n",
      "xtest int32\n",
      "ytrain int32\n",
      "ytest int32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.55      0.54      2453\n",
      "           1       0.43      0.46      0.45      2357\n",
      "           2       0.51      0.45      0.48      2453\n",
      "           3       0.50      0.55      0.53      2364\n",
      "           4       0.39      0.33      0.36      2421\n",
      "           5       0.36      0.39      0.37      2352\n",
      "\n",
      "    accuracy                           0.46     14400\n",
      "   macro avg       0.45      0.46      0.45     14400\n",
      "weighted avg       0.45      0.46      0.45     14400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ExtraTreeClassifier\n",
      "training: ExtraTreeClassifier()\n",
      "xtrain int32\n",
      "xtest int32\n",
      "ytrain int32\n",
      "ytest int32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.33      0.34      2453\n",
      "           1       0.23      0.24      0.23      2357\n",
      "           2       0.25      0.26      0.26      2453\n",
      "           3       0.26      0.26      0.26      2364\n",
      "           4       0.19      0.18      0.19      2421\n",
      "           5       0.22      0.22      0.22      2352\n",
      "\n",
      "    accuracy                           0.25     14400\n",
      "   macro avg       0.25      0.25      0.25     14400\n",
      "weighted avg       0.25      0.25      0.25     14400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LinearSVC\n",
      "training: LinearSVC()\n",
      "xtrain int32\n",
      "xtest int32\n",
      "ytrain int32\n",
      "ytest int32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erris\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.48      0.35      2453\n",
      "           1       0.21      0.27      0.23      2357\n",
      "           2       0.23      0.25      0.24      2453\n",
      "           3       0.17      0.10      0.13      2364\n",
      "           4       0.21      0.10      0.13      2421\n",
      "           5       0.19      0.16      0.17      2352\n",
      "\n",
      "    accuracy                           0.23     14400\n",
      "   macro avg       0.22      0.23      0.21     14400\n",
      "weighted avg       0.22      0.23      0.21     14400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "KNeighbors\n",
      "training: KNeighborsClassifier()\n",
      "xtrain int32\n",
      "xtest int32\n",
      "ytrain int32\n",
      "ytest int32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.46      0.39      2453\n",
      "           1       0.24      0.36      0.29      2357\n",
      "           2       0.28      0.26      0.27      2453\n",
      "           3       0.30      0.23      0.26      2364\n",
      "           4       0.23      0.14      0.17      2421\n",
      "           5       0.24      0.20      0.22      2352\n",
      "\n",
      "    accuracy                           0.28     14400\n",
      "   macro avg       0.27      0.28      0.27     14400\n",
      "weighted avg       0.27      0.28      0.27     14400\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model[0])\n",
    "    print(Train_Model(model[1]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 5, 4, ..., 1, 2, 4])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.55      0.54      2453\n",
      "           1       0.43      0.46      0.45      2357\n",
      "           2       0.51      0.45      0.48      2453\n",
      "           3       0.50      0.55      0.53      2364\n",
      "           4       0.39      0.33      0.36      2421\n",
      "           5       0.36      0.39      0.37      2352\n",
      "\n",
      "    accuracy                           0.46     14400\n",
      "   macro avg       0.45      0.46      0.45     14400\n",
      "weighted avg       0.45      0.46      0.45     14400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_XGBC = XGBClassifier()\n",
    "model_XGBC.fit(X_train,y_train)\n",
    "print(classification_report(y_test,model_XGBC.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3\n",
       "0    5\n",
       "0    2\n",
       "0    1\n",
       "0    0\n",
       "0    4\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df['category'].map(label_dic)\n",
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n",
    "    return labeled\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Text'],labels, random_state=0, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['malaysia', 'airlines', 'flight', 'mh', 'tragedy', 'keep', 'travelingits', 'never', 'easy', 'cope', 'death', 'innocents', 'let', 'alone', 'thousands', 'upon', 'millions', 'already', 'lost', 'politicallycharged', 'conflict', 'travelers', 'best', 'travel'], tags=['Train_0']),\n",
       " TaggedDocument(words=['facebook', 'suspends', 'political', 'research', 'firm', 'linked', 'trump', 'violating', 'user', 'privacythe', 'group', 'reportedly', 'obtained', 'personal', 'information', 'million', 'users'], tags=['Train_1'])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['malaysia', 'airlines', 'flight', 'mh', 'tragedy', 'keep', 'travelingits', 'never', 'easy', 'cope', 'death', 'innocents', 'let', 'alone', 'thousands', 'upon', 'millions', 'already', 'lost', 'politicallycharged', 'conflict', 'travelers', 'best', 'travel'], tags=['Train_0'])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4548</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>romney rubio many others called trump con man ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>personal playlist spring devising new playlist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6613</th>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>obama family attends church matching monochrom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>blac chyna rob kardashian taking things slow w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7775</th>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>jennifer aniston gets dolled ellen degeneres v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            category                                               Text\n",
       "4548        POLITICS  romney rubio many others called trump con man ...\n",
       "2591        WELLNESS  personal playlist spring devising new playlist...\n",
       "6613  STYLE & BEAUTY  obama family attends church matching monochrom...\n",
       "1308   ENTERTAINMENT  blac chyna rob kardashian taking things slow w...\n",
       "7775  STYLE & BEAUTY  jennifer aniston gets dolled ellen degeneres v..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_lengh = df['Text'].apply(lambda x:len(x.split())).max()\n",
    "max_lengh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48000/48000 [00:00<00:00, 4352913.28it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 4343053.59it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 3681434.20it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 3418863.11it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 3436545.68it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 3691153.62it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 3071297.04it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 2991657.63it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1493709.09it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1631244.72it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 2938559.55it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 1530348.99it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 2029113.29it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 2870721.82it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 4801950.87it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 4559748.87it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 4237473.26it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 3716364.09it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 2879796.77it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 3419734.20it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 3467381.84it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 3614416.11it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 3693320.47it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 3947967.29it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 3948431.86it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 3542548.82it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 3175548.38it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 3352816.83it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 3539559.27it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 2170262.73it/s]\n",
      "100%|██████████| 48000/48000 [00:00<00:00, 3319865.31it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=max_lengh, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x1b00f2f5bb0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erris\\AppData\\Local\\Temp\\ipykernel_1736\\504554781.py:13: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  vectors[i] = model.docvecs[prefix]\n"
     ]
    }
   ],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors\n",
    "    \n",
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), max_lengh, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), max_lengh, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def Train_Model(model):\n",
    "    model.fit(train_vectors_dbow,y_train)\n",
    "    return print(classification_report(y_test,model.predict(test_vectors_dbow)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.55      0.56      2424\n",
      "           1       0.42      0.40      0.41      2399\n",
      "           2       0.49      0.49      0.49      2359\n",
      "           3       0.46      0.49      0.47      2377\n",
      "           4       0.41      0.40      0.41      2432\n",
      "           5       0.50      0.49      0.50      2409\n",
      "\n",
      "    accuracy                           0.47     14400\n",
      "   macro avg       0.47      0.47      0.47     14400\n",
      "weighted avg       0.47      0.47      0.47     14400\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "RandomForest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82      2424\n",
      "           1       0.67      0.70      0.69      2399\n",
      "           2       0.79      0.77      0.78      2359\n",
      "           3       0.74      0.71      0.72      2377\n",
      "           4       0.69      0.63      0.66      2432\n",
      "           5       0.76      0.79      0.77      2409\n",
      "\n",
      "    accuracy                           0.74     14400\n",
      "   macro avg       0.74      0.74      0.74     14400\n",
      "weighted avg       0.74      0.74      0.74     14400\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "ExtraTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.44      0.45      2424\n",
      "           1       0.35      0.34      0.35      2399\n",
      "           2       0.39      0.40      0.39      2359\n",
      "           3       0.37      0.40      0.38      2377\n",
      "           4       0.33      0.32      0.32      2432\n",
      "           5       0.37      0.37      0.37      2409\n",
      "\n",
      "    accuracy                           0.38     14400\n",
      "   macro avg       0.38      0.38      0.38     14400\n",
      "weighted avg       0.38      0.38      0.38     14400\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "XGBRFClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.76      0.73      2424\n",
      "           1       0.59      0.57      0.58      2399\n",
      "           2       0.68      0.66      0.67      2359\n",
      "           3       0.63      0.62      0.63      2377\n",
      "           4       0.60      0.55      0.58      2432\n",
      "           5       0.66      0.70      0.68      2409\n",
      "\n",
      "    accuracy                           0.65     14400\n",
      "   macro avg       0.64      0.65      0.64     14400\n",
      "weighted avg       0.64      0.65      0.64     14400\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "XGBClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85      2424\n",
      "           1       0.74      0.74      0.74      2399\n",
      "           2       0.84      0.81      0.83      2359\n",
      "           3       0.77      0.78      0.77      2377\n",
      "           4       0.74      0.74      0.74      2432\n",
      "           5       0.81      0.83      0.82      2409\n",
      "\n",
      "    accuracy                           0.79     14400\n",
      "   macro avg       0.79      0.79      0.79     14400\n",
      "weighted avg       0.79      0.79      0.79     14400\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "ExtraTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.48      0.48      2424\n",
      "           1       0.35      0.35      0.35      2399\n",
      "           2       0.41      0.42      0.42      2359\n",
      "           3       0.36      0.38      0.37      2377\n",
      "           4       0.33      0.32      0.32      2432\n",
      "           5       0.38      0.37      0.38      2409\n",
      "\n",
      "    accuracy                           0.38     14400\n",
      "   macro avg       0.38      0.38      0.38     14400\n",
      "weighted avg       0.38      0.38      0.38     14400\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "LinearSVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erris\\miniconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85      2424\n",
      "           1       0.77      0.73      0.75      2399\n",
      "           2       0.84      0.82      0.83      2359\n",
      "           3       0.78      0.79      0.78      2377\n",
      "           4       0.75      0.75      0.75      2432\n",
      "           5       0.81      0.85      0.83      2409\n",
      "\n",
      "    accuracy                           0.80     14400\n",
      "   macro avg       0.80      0.80      0.80     14400\n",
      "weighted avg       0.80      0.80      0.80     14400\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "KNeighbors\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.92      0.73      2424\n",
      "           1       0.83      0.33      0.47      2399\n",
      "           2       0.95      0.39      0.56      2359\n",
      "           3       0.40      0.84      0.54      2377\n",
      "           4       0.79      0.25      0.38      2432\n",
      "           5       0.64      0.82      0.72      2409\n",
      "\n",
      "    accuracy                           0.59     14400\n",
      "   macro avg       0.70      0.59      0.57     14400\n",
      "weighted avg       0.70      0.59      0.57     14400\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model[0])\n",
    "    print(Train_Model(model[1]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DecisionTree', DecisionTreeClassifier()),\n",
       " ('RandomForest', RandomForestClassifier()),\n",
       " ('ExtraTreeClassifier', ExtraTreeClassifier()),\n",
       " ('XGBRFClassifier',\n",
       "  XGBRFClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "                  colsample_bylevel=1, colsample_bytree=1,\n",
       "                  early_stopping_rounds=None, enable_categorical=False,\n",
       "                  eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "                  importance_type=None, interaction_constraints='', max_bin=256,\n",
       "                  max_cat_to_onehot=4, max_delta_step=0, max_depth=6,\n",
       "                  max_leaves=0, min_child_weight=1, missing=nan,\n",
       "                  monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "                  num_parallel_tree=100, objective='multi:softprob',\n",
       "                  predictor='auto', random_state=0, reg_alpha=0,\n",
       "                  sampling_method='uniform', scale_pos_weight=None, ...)),\n",
       " ('XGBClassifier',\n",
       "  XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "                colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "                early_stopping_rounds=None, enable_categorical=False,\n",
       "                eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "                importance_type=None, interaction_constraints='',\n",
       "                learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "                max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "                missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "                n_jobs=0, num_parallel_tree=1, objective='multi:softprob',\n",
       "                predictor='auto', random_state=0, reg_alpha=0, ...)),\n",
       " ('ExtraTreeClassifier', ExtraTreeClassifier()),\n",
       " ('LinearSVC', LinearSVC()),\n",
       " ('KNeighbors', KNeighborsClassifier())]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'max_len':max_len,\n",
    "    'tokenizer':tokenizer,\n",
    "    'encoder':enc,\n",
    "    'vectorzier':vectorizer,\n",
    "    'models':models,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models.pk']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(model_dict,\"models.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = load('models.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_len': 128,\n",
       " 'tokenizer': <keras_preprocessing.text.Tokenizer at 0x1b06b8de5e0>,\n",
       " 'encoder': LabelEncoder(),\n",
       " 'vectorzier': TfidfVectorizer(),\n",
       " 'models': [('DecisionTree', DecisionTreeClassifier()),\n",
       "  ('RandomForest', RandomForestClassifier()),\n",
       "  ('ExtraTreeClassifier', ExtraTreeClassifier()),\n",
       "  ('XGBRFClassifier',\n",
       "   XGBRFClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "                   colsample_bylevel=1, colsample_bytree=1,\n",
       "                   early_stopping_rounds=None, enable_categorical=False,\n",
       "                   eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "                   importance_type=None, interaction_constraints='', max_bin=256,\n",
       "                   max_cat_to_onehot=4, max_delta_step=0, max_depth=6,\n",
       "                   max_leaves=0, min_child_weight=1, missing=nan,\n",
       "                   monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "                   num_parallel_tree=100, objective='multi:softprob',\n",
       "                   predictor='auto', random_state=0, reg_alpha=0,\n",
       "                   sampling_method='uniform', scale_pos_weight=None, ...)),\n",
       "  ('XGBClassifier',\n",
       "   XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "                 colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "                 early_stopping_rounds=None, enable_categorical=False,\n",
       "                 eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "                 importance_type=None, interaction_constraints='',\n",
       "                 learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "                 max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "                 missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "                 n_jobs=0, num_parallel_tree=1, objective='multi:softprob',\n",
       "                 predictor='auto', random_state=0, reg_alpha=0, ...)),\n",
       "  ('ExtraTreeClassifier', ExtraTreeClassifier()),\n",
       "  ('LinearSVC', LinearSVC()),\n",
       "  ('KNeighbors', KNeighborsClassifier())]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b8a52038f5da5aa877987515cd692d8371a6f143b75c12d63db471be6b9a2e4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
